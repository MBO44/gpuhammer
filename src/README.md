# GPU-RowHammer
## Required Environment
- Anaconda 24.9.2 (Note: most software requirements are resolved by the RAPIDS RMM conda environment)
- CMake 3.26.4+
- C++17+
- NVIDIA GPU sm_80+
- Python 3.10+
- NVIDIA CUDA Driver (Tested with Driver Version: 545.23.08)
- NVIDIA CUDA Toolkit (Tested with CUDA Version: 12.3)

## Affected GPUs
- NVIDIA A6000 GPU with 48GB GDDR6

## Build

1. Setup conda and RAPIDS RMM development environment and libraries with `bash env_setup.sh`
2. Enable conda virtual environment with `conda activate rmm_dev`
3. Run `cmake -S . -B ./out/build`
4. In `./out/build`, run `make`

Feel free to edit the CMakeLists.txt to change build configurations. Simply do step 4 when you make any changes to apply them.

## GPU Setup
For the Rowhammer attack, a prerequiste is disabling ECC, if that is not already done by default on the GPU. Additionally, our profiling is easier with the persistence mode enabled, and with fixed GPU and memory clock rates, although these are not pre-requisites. The following script performs the above actions:
```bash
# Example usage: 
#   bash init_cuda.sh 1800 7600
bash init_cuda.sh <MAX_GPU_CLOCK> <MAX_MEMORY_CLOCK>
```

**MAX_GPU_CLOCK** and **MAX_MEMORY_CLOCK** are found with `deviceQuery` from CUDA samples, which we provide a sample for A6000 in 'deviceQuery.txt'. 

To undo the changes, run:
```bash
# Note this script don't reset ECC state.
bash reset_cuda.sh
```

## General Steps of usage
<!-- To understand some of the terminologies, gloss over my Report in [GitHub](https://github.com/ShaopengLin/GPU-RowHammer/blob/main/Row_Hammer_Summary.pdf) or [Typst](https://typst.app/project/rAVAM0FL3MxzFRAiYP29Cd). -->

### Step 1: Obtain Row Addresses to Hammer

#### Row Buffer Conflict Tools 
In the repository root folder:
```python
# Display available tasks available for row buffer conflict
python3 run_timing_task.py -h

# Display usage to the specific task
python3 run_timing_task.py <task> -h
```

1. Get a rough idea of the **tRC** (time between two row activations) with the output of `gt` task. Below you should be able to observe the difference in latency between row-buffer hits and conflicts. By default, the result is stored in a text `TIME_VALUE.txt` generated by the script.
   - ```python3 run_timing_task.py gt```
   -  ```
      (0x7f1b28000000, 0x7f1b282affa0, )	328
      (0x7f1b28000000, 0x7f1b282affc0, )	336
      (0x7f1b28000000, 0x7f1b282affe0, )	327
      (0x7f1b28000000, 0x7f1b282b0000, )	338
      (0x7f1b28000000, 0x7f1b282b0020, )	360
      (0x7f1b28000000, 0x7f1b282b0040, )	360
      ```
2. Obtain a **Conflict Set** with the output of `conf_set` task
Conflict set is a list of array offsets (addresses) that map to rows that conflict with the row of a fixed address (reference). By default, the result is stored in a text `CONF_SET.txt` generated by the script.

   - ```python3 run_timing_task.py conf_set```
   -  ```
      851968
      852480
      868608
      869120
      894976
      895488
      911616
      912128
      ```
3. Obtain the **Row Set** for the bank corresponding to step 2 with the output of `row_set` task. 
Row Set is a matrix of offsets, where each line lists the address offsets mapping to the same DRAM row. By picking one adddress from each line, we can map to unique rows in the DRAM bank. By default, the result is stored in a text `ROW_SET.txt` generated by the script.

   - ```python3 run_timing_task.py row_set CONF_SET.txt```
   -  ```
        851968  852480  868608  ...
        1153024  1153536  1169664  ...
        ...
      ```
4. (Optionally) Obtain a **Bank Set** with the output of `bank_set` task. Bank Set has the offsets that correspond to different banks in the GPU. It is recommended that you add a large _size_ and test a small _max_ at first, as the number of banks is undecidable for this program. By default, the result is stored in a text `BANK_SET.txt` generated by the script.
   - ```python3 run_timing_task.py bank_set --max <max> --size <size>```
   -  ```
      0
      256
      1024
      1280
      2048
      ```

### Step 2: Hammer

1. Obtain the **Optimal Synchronization Delay Configuration**. This step helps to obtain the exact delay to be added to synchronize the aggressor pattern with DRAM REF commands. A helpful script, `run_delay.sh`, is available in the root folder. We assume the row set files are stored at `./row_sets/ROW_SET_<bank_id>.txt`, and a `log` folder is present for dumping the hammering results. Please update the relevant variables in the script, specifically the `bank_id` and file locations. You can run the script with

   ```bash
   sudo bash run_delay.sh
   ```

   You can visualize the result by running 
   ```bash
   python3 plot_delay.py <iterations> <trefi> <bank_id> <num_aggressors>
   ```

   The first parameter is the number of iterations, which should match the `iterations` variable in the bash script. The second parameter is the tREFI period in nanoseconds, which is 1407 for the A6000. The fourth and onward parameters specify the number of aggressors. You can plot multiple aggressor configurations on the same plot.

   On the output plot, observe that there is a flat-lined area in the middle. This is where synchronization occurs, and the delays in that area are optimal.


2. **Start Hammering!** The helper script `run_hammer.sh` is available in the root folder. Again, we assume a file structure for the row set files and log files, so please update the relevant variables. In addition, fill in the `delay` configuration you obtained from the previous step. You can run the script with
   ```bash
   sudo bash run_hammer.sh
   ```

   The result of your hammering can be found at `log/<#_of_aggressors>agg_b<bank_id>.log`. Any bitflips will be reported in the form `Observed Bit-Flip ...` in the log file.

### Campaign Configurations

In our systematic study, we ran attacks with `num_agg = 8, 12, 16, 20, 24` and `skip_step = 3`. We launched campaigns with both gridded victim data patterns. (`vic_pat=55, agg_pat=aa` and `vic_pat=aa, agg_pat=55` ) For each configuration, we used the following setups:
- **8-sided**: 8 warps, 1 thread, 3 rounds
- **12-sided**: 6 warps, 2 threads, 2 rounds
- **16-sided**: 8 warps, 2 threads, 1 round
- **20-sided**: 10 warps, 2 threads, 1 round
- **24-sided**: 8 warps, 3 threads, 1 round

Each hammer takes approximately 1200 ms to run if we treat the entire bank as victim rows. Alternatively, to speed up the hammers, you can comment and uncomment some code according to line 108 of `hammer/gpu_hammer.cu`.
This verifies only the rows in the neighborhood of the hammered aggressors for bitflips, reducing the time for a hammer kernel to around 200 ms. With this setup, it takes approximately 5 hours to complete a sweep on one bank with a single `num_agg` configuration for both data patterns.

### Step 3: Exploit
Once you have found some bit-flips, you can run an exploit, targeting these bit-flips in a ML model weight. 
The following script will run the exploit on ImageNet models with specific bitflips we found in our A6000 GPU (B1, B2, D1, and D3):
```bash
bash run_exploit.sh
```

This entire exploit with all the bit-flips should take one day.

#### Results

Accuracy Degradation is stored for each bitflip in their respective folder in `exploit/*`. We in addition generate a `degredation_table.csv` that records the highest RAD reported per model per bitflip.

You may find sample result in `sample_data` folder of each bitflip folder, a sample csv `exploit/sample_table.csv`, and a sample ASCII visualization of the csv table `exploit/sample_table_vis.txt`.

## General Tips:
1. The row buffer conflict code is hard to understand on its own. I suggest starting with `rbce_gen_time_main.cu` and working slowly through the code and how the GPU kernels work.
2. To specify a large amount of memory in bytes:
   - GiB
      ```bash
      $((<count> * (1<<30)))
      ```
   - MiB
      ```bash
      $((<count> * (1<<20)))
      ```
3. To retrieve the bank identifier/offset from **Bank Set**, use:
   ```bash
   sed '<NUM>q;d' BANK_SET.txt
   ```

## Additional Notes

### Clock Rate Note
The clock rate output by the console is always the maximum clock rate. Set the lgc value in init_cuda to >= maximum clock rate and the GPU will adjust to maximum clock rate. We will need NVML later to retrieve dynamic clock rate but that isn't necessary right now.

### Compiler Settings Note
The most stable compiler setting is this, which disables all GPU kernel optimizations. **Xcicc** are the frontend pipelines and **Xptxas** are the backend pipelines (hence PTX). 
   ```cmake 
   set(CMAKE_CUDA_FLAGS "${CMAKE_CUDA_FLAGS} --generate-line-info -O3 -Xcicc -O0 -Xptxas -O0")
   ```
This is the setting that gives the lowest latency while not optimizing away our memory accesses. **Xptxas** gives the most performance increase since we are mostly using PTX and register allocation is our biggest concern. Turning either one past **O3** on its own will not optimize the access away, but with the other one past **O2**, it will be gone. They must have some mechanism at work collaborating the two, but it is not a major concern to us. 
   ```cmake 
   set(CMAKE_CUDA_FLAGS "${CMAKE_CUDA_FLAGS} --generate-line-info -O3 -Xcicc -O1 -Xptxas -O3")
   ```

## Happy Hammering :)


